---
title: Computació paral·lela amb cluster de Raspberry Pi
description: Taller de computació Paral·lela. Farem servir un cluster de raspberry Pi, programanat amb MPI amb Pytohn
marp: true
theme: default 
paginate: true
# backgroundImage: url('https://marp.app/assets/hero-background.jpg')
_paginate: false
_header: ''
_footer: ''
style: |
  section.centrar h1{
    text-align: center;
    font-size: 70px;
    background-color: lavender;
  }
  header, footer{ 
    font-size: 90%;
    text-align: center; 
  }
  p{
    font-size: 100%
  }
  section.menor p, table, ul{
    font-size: 80%
  }

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

<!-- _class: centrar -->

# Computació Paral·lela amb un clúster de Raspberry Pi. Programació amb `Python` i `MPI`. 
# Joan Gerard Camarena Estruch.

---
# Elements necessaris
## Hardware
- 4 raspberry Pi amb alimentador
- 4 microSD
- 1 Switch
- 1 rack per a Raspberry

## Software:
- MPI (Message Passing Interface)
- Python
- ssh 
![bg right](https://images.ctfassets.net/tvfg2m04ppj4/5LhT20PZvvANcXRAEvkd6v/cf8adcc5c652c0be55acbb7cabec806e/assembly_4.jpg)
---

# Pla de treball
1. Acoblament del clúster
2. Instal·lar i configurar la imatge bàsica (_master_)
3. Copia i configuració de la resta d'imatges (_nodes_)
4. Configuració del clúster. Carpetes en xarxa. ssh
5. MPI: Teoria i fonaments
6. Proves de progamació distribuida
---
# Acoblament del cluster
Teniu com a material:
- 4 rapsberrys Pi
- 4 carregadors 
- 4 microSD
- 5 cables RJ45
- 1 PC 
- 1 rack

Seguint les instruccions, atornillarem les plaquetes al rack i l'ensamblarem. 

---
# Imatge (1)
Descarregarem la imatge del sistema operatiu de la raspberry. 

Donat que ho configurarem tot per ssh i la terminal, no necessitarem cap escriptori.

La versió més actual és:
<https://downloads.raspberrypi.org/raspios_lite_armhf/images/raspios_lite_armhf-2021-01-12/2021-01-11-raspios-buster-armhf-lite.zip>

---

# Imatge (2)

El fitxer `img` resultant el tenim que escriure a la microSD. Per a fer-ho podem fer servir distints programes:

- [Windows] → `Win32ImageWriter`, disponible en <https://launchpad.net/win32-image-writer>
- [Ubuntu] → `usb-imagewriter`, dispoible als repositoris d'ubuntu
- Alternativa multiplataforma `balenaEtcher` <https://www.balena.io/etcher/> (sols grava a sd)

---

# Imatge (3)

Des de la consola en sistemes Linux/Mac, tant per gravar com per clonar:

1. Executar `sudo fdisk -l` i fixar-se dels discs que tenim al nostre sistema. en mac `diskutil list`
2. Inserir la microSD directament o amb algun acoplador. Es recomana a un port USB 3.
3. Tornem a executar `sudo fdisk -l`. Fixar-se en el tamnay (8 o 16GB). Serà algun disk que no teniem al pas 1. Suposarem que s'ha muntat en `/dev/sdx`. 
4. En cas que linux l'haja muntada en algun lloc, haurem de desmuntar-la `sudo umount /dev/sdx`. Pot-ser bote algun error que el disc no està muntat; no passa res, és el que voliem
5. I finalment, a la carpeta on tenim el fitxer: (`if` i `of` són input/output file)
```
sudo dd bs=16M if=arxiu.img of=/dev/sdx
```

---

# Configuració comuna (1)
1. Arrancar una raspberry (la 1) amb monitor i teclat connectat. Tindrà xarxa per DHCP.
2. `sudo apt update` , `sudo apt upgrade`
3. Executem `sudo raspi-config` i passar per les següents parts:
   1. Menú `1 System Options`:
    - `S4 Hostname` → `node1`
    - `S5 Boot/Autologin` → triar opció `B2 Console Autologin`
  2. Menú `2 Display Option`: Ací no tocarem res
  3. Menú `3 Interface Options`:
     - `P2 SSH` i habilitarem el login remot i el servidor SSH
  4. Menú `4 Performance Options`:
     1. `P2 GPU Memory` i en deixem sols 16MB. Pensa que sols entrarem en la consola
---

# Configuració comuna (2)

  5. Menú `5 Localisation`:
     1. `L1 Locale`: llevarem `en_GB.UTF8` i posarem el `es_ES.ETUF-8`
     2. `L2 Timezone`: posarem Europe -> Madrid
     3. `L3 Keyboard`: seleccionarem Spanish
  6. Menú `6 Advanced Options`:
     1. `A1 Expand Filesystem` -> per ocupar tota la sd
     2. `A4 Network Interface Names` -> seleccionem `no`. Per tenir eth0 en compte de `enxb827eb9f90a4` (unac combinació del literal `enx` seguit de la MAC de la interfícies) 
  7. Menú `8 Update` no cal res, ja que hem fet el `update-upgrade` abans
  8. Menú `9 About` no cal res
4. `sudo reboot` 

---

# Configuració comuna. Instalar MPI (1)

1. `mkdir mpich3` La carpeta que contindrà l'instalador
2. `cd mpich3` Entrem i descarreguem la versió
3. `wget http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz`
4. Descomprimim: `tar xfz mpich-3.3.2.tar.gz` 
5.  `sudo mkdir /home/rpimpi`
6.  `sudo mkdir /home/rpimpi/mpi-install`
7.  `sudo mkdir /home/pi/mpi-build`
8.  `cd /home/pi/mpi-build`
9.  `sudo apt-get install gfortran`

---

# Configuració comuna. Instalar MPI (2)

Compilem MPI per a la pataforma. Aquest procés pot durar fins 1 hora

1.  `sudo /home/pi/mpich3/mpich-3.2.2/configure -prefix=/home/rpimpi/mpi-install`
2.  `sudo make` → compilem
3.  `sudo make install`  → instalem
4.  afegim mpi al path: `nano .bashrc` en l'ultima linia `PATH=$PATH:/home/rpimpi/mpi-install/bin`
5.  reiniciar → `sudo reboot`
6.  Comprovar que mpi funciona: `mpiexec -n 1 hostname`

---

# Configuració comuna. Instalar MPI (3)
 
Instalem les llibreries de per a programar en Pytohn

1.  Instal·lar les llibreries extra de python `sudo apt install python-dev`
2.  Instal·lar el gestor de llibreries → `sudo apt install python-pip`
3.  Instal·lar mpi4py → `pip install mpi4py`
4.  Provem que funciona → `mpiexec -n 4 python helloworld.py`

```python
# -*- coding: utf-8 -*-
from mpi4py import MPI
import sys

size=MPI.COMM_WORLD.Get_size()
rank=MPI.COMM_WORLD.Get_rank()
name=MPI.Get_processor_name()

print("Hello world, Soc el procés %2d de %2d al node %10s" %(rank,size,name))
```
---

# Clonació de la targeta SD.

En aquets apartat el que correspon és :

1. Apagar la Raspberry
2. Fer un fitxer img a partir de la microSD
3. A partir del nou img gravar les altres 3 targetes.

Es subministra un fitxer img creat amb tota la instal·lació ja creada. Per tant la primera part de configuració **no caldrà fer-la**

---

# Configuración de cada node - Tasques

Hem de configurar en cada node:

1. Adreça estàtica
2. Hostname
3. Crear clau *ssh*
4. Transferir la clau entre tots els nodes
5. Muntar una carpeta compartida amb *nfs*

>Notes:
> - Necessitarem un _sniffer_ de la xarxa per descobrir quines IP's tenen. Es recomana <https://angryip.org>.
> - Per treballar amb totes les màquines alhora, podem fer servir alguna implementació de _cluster-ssh_

---

# Configuración de cada node (1)

Descobrir quines adreces tenen ens nodes

![center](DeteccioCluster.png)

---

# Configuración de cada node (2)

Amb alguna ferramenta com cluster-ssh ens connectem a elles
![height:200px center](cssh.png)

- Hem de crear un fitxer de text amb les IP's que hem trobat anteriorment
- Haurem de escrure un `yes` per acceptar el _fingerprint_
- Escriure el password `raspberry`

---

# Configuración de cada node (3)

![height:600px center](cssh2.png)

---

# Configuración de cada node (4)

## Adreça estàtica - `/etc/network/interafaces`

```bash
# source-directory /etc/network/interfaces.d

auto eth0
#iface eth0 inet dhcp
iface eth0 inet static
address 192.168.10.5x
netmask 255.255.255.0
gateway 192.168.10.1
```
> Notes:
> - La primera linia la comentarem, així com afegirem una comentada per dhcp
> - la `x` serà del 1..4. Desprès segons el valor de x li donarem nom al hostame
> - canviar `192.168.10` per la xarxa de l'aula adequada

---

# Configuración de cada node (5)

## Nom del host - `/etc/hostname`

Dins de cada fitxer contindrà el seu nom de host.
```
192.168.10.51  ←→ node1
192.168.10.52  ←→ node2
192.168.10.53  ←→ node3
192.168.10.54  ←→ node4
```
---

# Configuración de cada node (6)

## Equips del clúster - `/etc/hosts`

Aquest fitxer conté el llistat de tots els nodes del clúster, amb el seu nom i la seua IP. Això ens servirà per gastar noms en compte d'adreces.

```
127.0.0.1       localhost
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters

192.168.10.51   node1
192.168.10.52   node2
192.168.10.53   node3
192.168.10.54   node4
```

Reiniciar tot el cluster i fer proves de `ping`. Hauras de netejar el teu fitxer `~/.ssh/known_hosts` ja que hem canviat la IP

---

# Primera prova d'execució (1)

Anem a treballar sols amb el node1

1. Crec carpeta mpi_test: `mkdir mpi_test`
2. Moure el helloworld: `mv helloworldmpi.py mpi_test/`
3. Entrem i crear el següent `machinefile`. Aquest fitxer conte una linia per cada node del cluster, amb la sintaxi `node[:cores]`

```
node1:4
node2:4
node3:4
node4:4
```

---

# Primera prova d'execució (2)

```
pi@node1:~/mpi_test $ mpiexec -f machinefile -n 4 python helloworldmpi.py 
Hello world, I'm process 0 of 4 in processor node1
Hello world, I'm process 1 of 4 in processor node1
Hello world, I'm process 2 of 4 in processor node1
Hello world, I'm process 3 of 4 in processor node1
```
Tot be, ja que cada node té 4 cores, i s'executa tot en el node1

```
pi@node1:~/mpi_test $ mpiexec -f machinefile -n 16 python helloworldmpi.py 
Host key verification failed.
Host key verification failed.
Host key verification failed.
```

Falla. No tenim accès a la resta de nodes. MPI es comunica per `ssh`

---

# Configuración de cada node (7)

1. Hem d'aconseguir poder fer login automàticament desde qualssevol node a qualssevol altre node. 
2. Per aixó farem en `nodex (x=1..4)`
```
# generem les claus SSH
ssh-keygen                  # acceptar tot per defecte
cd ~/-.ssh
cp id_rsa.pub node_x_key    # x=1..4
```

Exportem en tots els nodes les claus dels altres tres. Exemple de exportació de la clau de node1 all altres tres

```
scp node1:/home/pi/.ssh/node1_key .     #vigila el últim .
cat node_1_key>>authorized_keys
```

---

# Configuración de cada node (8)

![center](ssh_keys.png)

En `authorized_keys` tenim les claus ssh dels altres tres nodes. Ja podem entrar (login) sense problema

---

# Primera prova d'execució (3)

Tornem al prova, des del `node1`

```
pi@node1:~/mpi_test $ mpiexec -f machinefile -n 16 python helloworldmpi.py 
[proxy:0:3@node4] launch_procs ... /home/pi/mpi_test (No such file or directory)
```

El que indica el següent missatge és que quan estavem en node4, ha intentat entrar en la carpeta `/home/pi/mpi_test` i no la troba.

### Problema
El codi font ha d'estar en la mateixa ruta en tels nodes

### Possibles solucions:
  - Cada cop que editem el codi, l'haurem de copiar en totes els nodes
  - Creem una carpeta en xarxa, compartida per tots els nodes per NFS. **Aquesta és la bona** :+1:

---

# Configuración de cada node (9)

La manera de treball serà:

**node1**:
- Executarà un servidor de nfs, i exportarem (compartirem) una carpeta per xarxa
- Com a programadors treballarem exclusivament en aquest node
 
**resta de nodes**:
- Monterem a l'arranc la carpeta en xarxa
- Estaran en funcionament, però no treballarema amb ells. Seràn invocats per node1 

---

# Configuración de cada node (10)

## Instalar servidor nfs i compartir carpeta - (en node1) 

- `sudo apt install nfs-kernel-server`
- `mkdir /home/pi/cloud`
- `sudo nano /etc/exports` i afegir 
```
/home/pi/cloud *(rw,sync,no_root_squash,no_subtree_check)
```
- `sudo chmod -R 777 /home/pi/cloud`
- `sudo update-rc.d rpcbind enable`
- `sudo service rpcbind restart`
- `sudo exportfs -a`
- `sudo service nfs-kernel-server restart`

---

# Configuración de cada node (11)

## Instalar client nfs i montar carpeta - (en `nodex, x=2,3,4`) 

- `sudo mkdir /home/pi/cloud`
- `sudo /etc/fstab` i afegir la linia: 
```
node1:/home/pi/cloud /home/pi/cloud  nfs auto,noatime,nolock,bg,nfsvers=4,intr,actimeo=1800 0 0`
```
- reset en tots els `nodex` per comprovar que automonta

![height:200px center](df-h.png)

---

# Primera prova d'execució (4)

- En node1 movem el programa `helloworldmpi.py` i el `machinefile` de `mpi_test` a `cloud`.
```
pi@node1:~/cloud $ mpiexec -f machinefile -n 16 python helloworldmpi.py 
Hello world, Soc el procés  0 de 16 a      node1
Hello world, Soc el procés  8 de 16 a      node3
Hello world, Soc el procés 12 de 16 a      node4
Hello world, Soc el procés  4 de 16 a      node2
Hello world, Soc el procés  1 de 16 a      node1
Hello world, Soc el procés  9 de 16 a      node3
Hello world, Soc el procés 13 de 16 a      node4
Hello world, Soc el procés  5 de 16 a      node2
Hello world, Soc el procés  2 de 16 a      node1
Hello world, Soc el procés 10 de 16 a      node3
Hello world, Soc el procés 14 de 16 a      node4
Hello world, Soc el procés  6 de 16 a      node2
Hello world, Soc el procés  3 de 16 a      node1
Hello world, Soc el procés 11 de 16 a      node3
Hello world, Soc el procés 15 de 16 a      node4
Hello world, Soc el procés  7 de 16 a      node2
```
---

# Primera prova d'execució (i 5)

Observem que:

- S'han executat 4 tasques en cada node, gràcies a la configuració del `machinefile` (recordes `node1:4`?)
- El funcionament és anàrquic: molts processos fent coses alhora, i no hi ha control del que ix alhora 
- Fes distintes proves en múltiples de 4

# Fi de la primera part

---

<!-- _class: centrar -->

# Fi de la primera part
# Tornem i a programar...